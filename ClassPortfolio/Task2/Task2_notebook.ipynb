{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Pacific SST Principle Component Analysis and Local Forecasts\n",
    "\n",
    "---\n",
    "\n",
    "This is Matt Pudig's notebook for MATH5271 class portfolio task 2 draft. \n",
    "\n",
    "In this notebook, we load and analyse both the 2D SST field from ERSSTv3b from years 1960 to 2016, as well as the complete dataset available from from the _Pacific Sea Level Monitoring Project_ for the Pacific island nation, Tonga.\n",
    "\n",
    "---\n",
    "\n",
    "_\"This task should include 3-4 original figures and approximately 1000 words of text. Your draft should include a solid attempt at parts 1 and 2 and at least a proposed outline of part 3. (The closer your draft is to complete the more useful your peer feedback will be).\"_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone in data:\n",
    "\n",
    "# %%bash\n",
    "# # Mounting requires a special 'bash' command\n",
    "# git clone https://github.com/pangeo-data/tutorial-data.git\n",
    "\n",
    "ERSST = xr.open_mfdataset('./tutorial-data/sst/*nc', combine='by_coords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 2.1\n",
    "---\n",
    "2.1 _\"Analyse both the 2D (latitude, longitude, time) data from the global Sea Surface Temperature (SST) data set used in the Week 4 Lab session and the entire set of years available from the  Pacific Sea Level Monitoring Project for the location you used in Task  1 (i.e. you will need to download more years). Discuss the differences in data provenance and compare the sea water temperature data gathered in situ to the values of the 2D data set at the nearest available data point.\"_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Talk about data provenance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b457c96e48f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# Load in _all_ csv files from my public github repository: https://github.com/mpudig/MATH5271/tree/main/ClassPortfolio/Task1\n",
    "\n",
    "import glob\n",
    "\n",
    "def read_multi_csv(start_year,end_year):     \n",
    "    years = list(range(start_year,end_year+1))     \n",
    "    dfs = []\n",
    "    for YYYY in years:         \n",
    "        file = 'https://raw.githubusercontent.com/username/project/main/data/normalized/'+str(YYYY)+'_crimes_byState.csv'             \n",
    "        #print (file)         \n",
    "        df = pd.read_csv(file)         \n",
    "        dfs.append(df)\n",
    "    all_dfs = df.concat(df)         \n",
    "    return all_dfs  \n",
    "\n",
    "read_multi_csv(2013,2019)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \"Consider how the area of the 'grid cells' of the SST data used changes with latitude and consider accounting for this in the determination of your PCs.\" Could multiply each SST by the cosine of the latitude, compute the PCs, and then divide the PCs by the cosine of the latitude? (As done in Smith et al. 1996, top of p.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greta: \"So the idea is to compute principle components of SST for a subset of the global data containing our weather station of interest, then use this data as a predictor for SST at our weather station of interest?\"\n",
    "\n",
    "Jan: \"Yeah yeah that's the idea. Compute the PCs. So you're basically developing a linearised model of the SST, and seeing if that can be used to predict the temperature at the actual tide guage stations.\"\n",
    "\n",
    "Greta: \"And to develop that model, you'd look at the temperature at the tide guage station, and try to fit your PC model to that?\"\n",
    "\n",
    "Jan: \"Yep yep, well... Your PC model would just be based on the SST data. And so, if you take the whole Tropical Pacific, you'll be looking at the whole PCs for that region. And then you're developing a model that is somewhat unsupervised of the whole temperature record, but you're interested in what's happening at a particular station. The idea is to have a large enough region to have nice principal components, and have some nice spatial maps for the eigenvalues fo that system, and then see how well these model the tide guage.\"  \n",
    "\n",
    "E.g., if you only have four principal components, you're only relating the temperature to four variables. So you need to think about...\n",
    "\n",
    "\"What you're wanting to do: If you only had the four PCs, how well woudl you be able to predict the temeprature at your tide guage station\"\n",
    "\n",
    "\"Station data is predictand, and the EOFs are predictors (or, temperatures represented by the EOFs)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
